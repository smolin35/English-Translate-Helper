{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'In our everyday lives, artificial neural networks are already extensively \\\n",
    "used, for example, for carrying out tasks such as face and speech recog- \\\n",
    "nition, which are frequently performed on our mobile phones1. In more \\\n",
    "complex applications, such as medical diagnostics2 and autonomous \\\n",
    "driving1,3, high-speed data analysis will become ever more important. \\\n",
    "However, fulfilling this demand for fast and efficient processing using \\\n",
    "traditional computation techniques is problematic, owing to speed and \\\n",
    "energy inefficiencies4. Traditional computers are built following the \\\n",
    "von Neumann architecture, with two separate units for memory and \\\n",
    "processor operating in a sequential way, one command at a time. When \\\n",
    "compared to the massively parallel signal processing of the brain, it \\\n",
    "is clear why simulating a neural network in software on a machine \\\n",
    "based on the von Neumann architecture and limited by the transfer of \\\n",
    "data between memory and processor cannot be efficient5.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', \"''\", '(', ')', ',', '--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "\n",
      "r_all==r_by_word = False\n",
      "\n",
      "[('In', 'IN'), ('our', 'PRP$'), ('everyday', 'JJ'), ('lives', 'NNS'), (',', ','), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('are', 'VBP'), ('already', 'RB'), ('extensively', 'RB'), ('used', 'VBN'), (',', ','), ('for', 'IN'), ('example', 'NN'), (',', ','), ('for', 'IN'), ('carrying', 'VBG'), ('out', 'RP'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('face', 'NN'), ('and', 'CC'), ('speech', 'JJ'), ('recog-', 'JJ'), ('nition', 'NN'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('frequently', 'RB'), ('performed', 'VBN'), ('on', 'IN'), ('our', 'PRP$'), ('mobile', 'JJ'), ('phones1', 'NN'), ('.', '.'), ('In', 'IN'), ('more', 'RBR'), ('complex', 'JJ'), ('applications', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('medical', 'JJ'), ('diagnostics2', 'NN'), ('and', 'CC'), ('autonomous', 'JJ'), ('driving1,3', 'NN'), (',', ','), ('high-speed', 'JJ'), ('data', 'NN'), ('analysis', 'NN'), ('will', 'MD'), ('become', 'VB'), ('ever', 'RB'), ('more', 'RBR'), ('important', 'JJ'), ('.', '.'), ('However', 'RB'), (',', ','), ('fulfilling', 'VBG'), ('this', 'DT'), ('demand', 'NN'), ('for', 'IN'), ('fast', 'JJ'), ('and', 'CC'), ('efficient', 'JJ'), ('processing', 'NN'), ('using', 'VBG'), ('traditional', 'JJ'), ('computation', 'NN'), ('techniques', 'NNS'), ('is', 'VBZ'), ('problematic', 'JJ'), (',', ','), ('owing', 'VBG'), ('to', 'TO'), ('speed', 'VB'), ('and', 'CC'), ('energy', 'NN'), ('inefficiencies4', 'NN'), ('.', '.'), ('Traditional', 'JJ'), ('computers', 'NNS'), ('are', 'VBP'), ('built', 'VBN'), ('following', 'VBG'), ('the', 'DT'), ('von', 'JJ'), ('Neumann', 'NNP'), ('architecture', 'NN'), (',', ','), ('with', 'IN'), ('two', 'CD'), ('separate', 'JJ'), ('units', 'NNS'), ('for', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('operating', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sequential', 'JJ'), ('way', 'NN'), (',', ','), ('one', 'CD'), ('command', 'NN'), ('at', 'IN'), ('a', 'DT'), ('time', 'NN'), ('.', '.'), ('When', 'WRB'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('massively', 'RB'), ('parallel', 'JJ'), ('signal', 'JJ'), ('processing', 'NN'), ('of', 'IN'), ('the', 'DT'), ('brain', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('clear', 'JJ'), ('why', 'WRB'), ('simulating', 'VBG'), ('a', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('in', 'IN'), ('software', 'NN'), ('on', 'IN'), ('a', 'DT'), ('machine', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('von', 'NN'), ('Neumann', 'NNP'), ('architecture', 'NN'), ('and', 'CC'), ('limited', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('transfer', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('between', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('efficient5', 'RB'), ('.', '.')]\n",
      "\n",
      "[('In', 'IN'), ('our', 'PRP$'), ('everyday', 'NN'), ('lives', 'NNS'), (',', ','), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('are', 'VBP'), ('already', 'RB'), ('extensively', 'RB'), ('used', 'VBN'), (',', ','), ('for', 'IN'), ('example', 'NN'), (',', ','), ('for', 'IN'), ('carrying', 'VBG'), ('out', 'IN'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('face', 'NN'), ('and', 'CC'), ('speech', 'NN'), ('recog-', 'NN'), ('nition', 'NN'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('frequently', 'RB'), ('performed', 'VBN'), ('on', 'IN'), ('our', 'PRP$'), ('mobile', 'NN'), ('phones1', 'NN'), ('.', '.'), ('In', 'IN'), ('more', 'RBR'), ('complex', 'JJ'), ('applications', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('medical', 'JJ'), ('diagnostics2', 'NN'), ('and', 'CC'), ('autonomous', 'JJ'), ('driving1,3', 'NN'), (',', ','), ('high-speed', 'NN'), ('data', 'NNS'), ('analysis', 'NN'), ('will', 'MD'), ('become', 'NN'), ('ever', 'RB'), ('more', 'RBR'), ('important', 'JJ'), ('.', '.'), ('However', 'RB'), (',', ','), ('fulfilling', 'VBG'), ('this', 'DT'), ('demand', 'NN'), ('for', 'IN'), ('fast', 'NN'), ('and', 'CC'), ('efficient', 'NN'), ('processing', 'NN'), ('using', 'VBG'), ('traditional', 'JJ'), ('computation', 'NN'), ('techniques', 'NNS'), ('is', 'VBZ'), ('problematic', 'JJ'), (',', ','), ('owing', 'VBG'), ('to', 'TO'), ('speed', 'NN'), ('and', 'CC'), ('energy', 'NN'), ('inefficiencies4', 'NN'), ('.', '.'), ('Traditional', 'JJ'), ('computers', 'NNS'), ('are', 'VBP'), ('built', 'NN'), ('following', 'VBG'), ('the', 'DT'), ('von', 'NN'), ('Neumann', 'NN'), ('architecture', 'NN'), (',', ','), ('with', 'IN'), ('two', 'CD'), ('separate', 'JJ'), ('units', 'NNS'), ('for', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('operating', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sequential', 'JJ'), ('way', 'NN'), (',', ','), ('one', 'CD'), ('command', 'NN'), ('at', 'IN'), ('a', 'DT'), ('time', 'NN'), ('.', '.'), ('When', 'WRB'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('massively', 'RB'), ('parallel', 'NN'), ('signal', 'NN'), ('processing', 'NN'), ('of', 'IN'), ('the', 'DT'), ('brain', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('clear', 'JJ'), ('why', 'WRB'), ('simulating', 'VBG'), ('a', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('in', 'IN'), ('software', 'NN'), ('on', 'IN'), ('a', 'DT'), ('machine', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('von', 'NN'), ('Neumann', 'NN'), ('architecture', 'NN'), ('and', 'CC'), ('limited', 'JJ'), ('by', 'IN'), ('the', 'DT'), ('transfer', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('between', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('efficient5', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/nik/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nik/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "  \n",
    "# Если пакет отсутствует, начните по его загрузке\n",
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagdict = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n",
    "# Выведем список тегов\n",
    "print(sorted(tagdict.keys()))\n",
    "print()\n",
    "s = 'It is expected that this type of reaction be used in 21st century. Built. Procesing'\n",
    "s = 'Traditional computers are built following the von Neumann architecture, with two separate units for memory and processor operating in a sequential way, one command at a time'\n",
    "s = 'The synapses are built of optical waveguides'\n",
    "s = text\n",
    "\n",
    "words = nltk.word_tokenize(s)\n",
    "\n",
    "# words = s.split(' ')\n",
    "\n",
    "r_all=nltk.pos_tag(words)\n",
    "\n",
    "r_by_word=[]\n",
    "for w in words:\n",
    "    r_by_word.append(nltk.pos_tag([w])[0])\n",
    "\n",
    "print(f'{r_all==r_by_word = }')\n",
    "print()\n",
    "print(r_all)\n",
    "print()\n",
    "print(r_by_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Для больших текстов эффективнее использовать pos_tag_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('In', 'IN'), ('our', 'PRP$'), ('everyday', 'JJ'), ('lives', 'NNS'), (',', ','), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('are', 'VBP'), ('already', 'RB'), ('extensively', 'RB'), ('used', 'VBN'), (',', ','), ('for', 'IN'), ('example', 'NN'), (',', ','), ('for', 'IN'), ('carrying', 'VBG'), ('out', 'RP'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('face', 'NN'), ('and', 'CC'), ('speech', 'JJ'), ('recog-', 'JJ'), ('nition', 'NN'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('frequently', 'RB'), ('performed', 'VBN'), ('on', 'IN'), ('our', 'PRP$'), ('mobile', 'JJ'), ('phones1', 'NN'), ('.', '.')], [('In', 'IN'), ('more', 'RBR'), ('complex', 'JJ'), ('applications', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('medical', 'JJ'), ('diagnostics2', 'NN'), ('and', 'CC'), ('autonomous', 'JJ'), ('driving1,3', 'NN'), (',', ','), ('high-speed', 'JJ'), ('data', 'NN'), ('analysis', 'NN'), ('will', 'MD'), ('become', 'VB'), ('ever', 'RB'), ('more', 'RBR'), ('important', 'JJ'), ('.', '.')], [('However', 'RB'), (',', ','), ('fulfilling', 'VBG'), ('this', 'DT'), ('demand', 'NN'), ('for', 'IN'), ('fast', 'JJ'), ('and', 'CC'), ('efficient', 'JJ'), ('processing', 'NN'), ('using', 'VBG'), ('traditional', 'JJ'), ('computation', 'NN'), ('techniques', 'NNS'), ('is', 'VBZ'), ('problematic', 'JJ'), (',', ','), ('owing', 'VBG'), ('to', 'TO'), ('speed', 'VB'), ('and', 'CC'), ('energy', 'NN'), ('inefficiencies4', 'NN'), ('.', '.')], [('Traditional', 'JJ'), ('computers', 'NNS'), ('are', 'VBP'), ('built', 'VBN'), ('following', 'VBG'), ('the', 'DT'), ('von', 'JJ'), ('Neumann', 'NNP'), ('architecture', 'NN'), (',', ','), ('with', 'IN'), ('two', 'CD'), ('separate', 'JJ'), ('units', 'NNS'), ('for', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('operating', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sequential', 'JJ'), ('way', 'NN'), (',', ','), ('one', 'CD'), ('command', 'NN'), ('at', 'IN'), ('a', 'DT'), ('time', 'NN'), ('.', '.')], [('When', 'WRB'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('massively', 'RB'), ('parallel', 'JJ'), ('signal', 'JJ'), ('processing', 'NN'), ('of', 'IN'), ('the', 'DT'), ('brain', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('clear', 'JJ'), ('why', 'WRB'), ('simulating', 'VBG'), ('a', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('in', 'IN'), ('software', 'NN'), ('on', 'IN'), ('a', 'DT'), ('machine', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('von', 'NN'), ('Neumann', 'NNP'), ('architecture', 'NN'), ('and', 'CC'), ('limited', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('transfer', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('between', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('efficient5', 'RB'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(text)\n",
    "s = [nltk.word_tokenize(s) for s in sents]\n",
    "print(nltk.pos_tag_sents(s, tagset=None, lang='eng')) # именные параметры приыведены для напоминания, что они есть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычитываем выделенные области из pdf файла\n",
    "\n",
    "text = [page0, page1, ...]   \n",
    "page = {'Пометка из аннотации', 'text'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['', '1h', '2']\n",
      "1 ['1', '2', '3', 'f1']\n",
      "2 ['1-pp', '2h', '3', '3-p', '4', '5', 'f1']\n",
      "3 ['1-pp', '2', '3', '3-p', 'f1']\n",
      "4 ['1-pp', '2h', '3', '4', 'f1']\n",
      "5 ['1-pp', '2', '2-p', '3h', '4', 'f1']\n",
      "6 ['1-pp', '2h', '3', '4', '5']\n",
      "7 ['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = 's41467-017-01506-3 - markup.pdf'\n",
    "file_name = Path(file_path).stem\n",
    "file_dir_path = Path(file_path).parent.resolve()\n",
    "\n",
    "\n",
    "doc = fitz.open(file_path)\n",
    "\n",
    "text = []\n",
    "for i,doc_page in enumerate(doc):\n",
    "    page = {}\n",
    "    for annot in doc_page.annots():\n",
    "        page[annot.info['content']] = doc_page.get_text(clip=annot.rect)\n",
    "    keys = sorted(page.keys())\n",
    "    print(f'{i}', keys)\n",
    "    text.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Объединяем разъединёные части\n",
    "- [x] части рисунков и обзацев на одной странице\n",
    "- [x] части абзаца на разных страницах\n",
    "    - [ ] Множество частей (часть части с предыдущей страницы) не поддерживается, нах =)\n",
    "\n",
    "Нодтация в общем:\n",
    "* n-pp: Текст является частью обзаца с предыдущей страницы\n",
    "* n-pm: Текст является часть обзаца с этой страцы, у самой первой части m(номер части) может отсутсвовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5\n",
      "3-p\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for page_i,page in enumerate(text):\n",
    "    keys = sorted(page.keys())\n",
    "    for i,k in enumerate(keys):\n",
    "        if len(k)>3 and (k[-2] == 'p' or k[-1] == 'p'):\n",
    "            if k[-2] == 'p': # part prev page\n",
    "                pp_i = page_i-1\n",
    "                pp_keysRevers = reversed(sorted(text[pp_i].keys()))\n",
    "                \n",
    "                pp_k = None\n",
    "                for pp_k in pp_keysRevers : \n",
    "                    if pp_k[0].isdigit():\n",
    "                        print(pp_k)\n",
    "                        break\n",
    "                text[pp_i][pp_k] = text[pp_i][pp_k] + page[k]\n",
    "                        \n",
    "                    \n",
    "            else: # part prev column\n",
    "                fi = k.rfind('-')\n",
    "                dest_k = k[:fi]\n",
    "                print(dest_k)\n",
    "                page[dest_k] = page[dest_k] + page[k] \n",
    "            page.pop(k, None)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенезируем\n",
    "\n",
    "tokenaise_text = [page0, page1, ...]  \n",
    "page = {'Пометка из аннотации', 'text'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenaise_text = []\n",
    "for page in text:\n",
    "    tokenaise_page = {}\n",
    "    keys = sorted(page.keys())\n",
    "    for k in keys:\n",
    "        sents = nltk.sent_tokenize(page[k])\n",
    "        s = [nltk.word_tokenize(s) for s in sents]\n",
    "        tokenaise_page[k] = nltk.pos_tag_sents(s, tagset=None, lang='eng')\n",
    "    tokenaise_text.append(tokenaise_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenaise_text[0])\n",
    "print(len(tokenaise_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Экспорт в html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1038@s41586-019-1157-8 - markup\n",
      "/home/nik/Work/AVT/Аспирантура/Английский/Translate Helper\n"
     ]
    }
   ],
   "source": [
    "print(file_name)\n",
    "print(file_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag():\n",
    "    def __init__(self, file, open_tag, close_tag = None):\n",
    "        self.file = file\n",
    "        self.open_tag = open_tag\n",
    "        self.close_tag = close_tag\n",
    "        if close_tag is None:\n",
    "            self.close_tag = f'</{self.open_tag[1:]}'\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.file.write(self.open_tag)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.file.write(self.close_tag)\n",
    "\n",
    "colors_parts = {'f': '#fffcde'}\n",
    "\n",
    "tagset = {'CC':'Coordinating conjunction',\n",
    "          'CD':'Cardinal number',\n",
    "          'DT':'Determiner',\n",
    "          'EX':'Existential there',\n",
    "          'FW':'Foreign word',\n",
    "          'IN':'Preposition or subordinating conjunction',\n",
    "          'JJ':'Adjective',\n",
    "          'JJR':'Adjective, comparative',\n",
    "          'JJS':'Adjective, superlative',\n",
    "          'LS':'List item marker',\n",
    "          'MD':'Modal',\n",
    "          'NN':'Noun, singular or mass',\n",
    "          'NNS':'Noun, plural',\n",
    "          'NNP':'Proper noun, singular',\n",
    "          'NNPS':'Proper noun, plural',\n",
    "          'PDT':'Predeterminer',\n",
    "          'POS':'Possessive ending',\n",
    "          'PRP':'Personal pronoun',\n",
    "          'PRP$':'Possessive pronoun',\n",
    "          'RB':'Adverb',\n",
    "          'RBR':'Adverb, comparative',\n",
    "          'RBS':'Adverb, superlative',\n",
    "          'RP':'Particle',\n",
    "          'SYM':'Symbol',\n",
    "          'TO':'to',\n",
    "          'UH':'Interjection',\n",
    "          'VB':'Verb, base form',\n",
    "          'VBD':'Verb, past tense',\n",
    "          'VBG':'Verb, gerund or present participle',\n",
    "          'VBN':'Verb, past participle',\n",
    "          'VBP':'Verb, non-3rd person singular present',\n",
    "          'VBZ':'Verb, 3rd person singular present',\n",
    "          'WDT':'Wh-determiner',\n",
    "          'WP':'Wh-pronoun',\n",
    "          'WP$':'Possessive wh-pronoun',\n",
    "          'WRB':'Wh-adverb',\n",
    "          '.':'.'}\n",
    "\n",
    "colors_tags = {'VB': '#ff0000',\n",
    "               'VBD': '#00ff00',\n",
    "               'VBG': '#0000ff',\n",
    "               'VBN': '#ff00ff',\n",
    "               'VBP': '#ffff00',\n",
    "               'VBZ': '#00ffff',\n",
    "               'RP': '#800000',\n",
    "               'MD': '#008000',\n",
    "               '.': '#000000'}\n",
    "\n",
    "def colorize_word(tagged_word):\n",
    "    keys = colors_tags.keys()\n",
    "    if tagged_word[1] in keys:\n",
    "        word = f'<text style=\"background-color: {colors_tags[tagged_word[1]]}\">{tagged_word[0]}</text>'\n",
    "        return word\n",
    "    return tagged_word[0]\n",
    "\n",
    "# # легенда\n",
    "# with open(f'{file_dir_path}/{file_name}-legend.html', 'w') as f:\n",
    "#         for c in colors_tags:\n",
    "\n",
    "def write_legend(file):\n",
    "    with Tag(file, '<pre>', '</pre>\\n') as t:\n",
    "        file.write('Легенда:\\n\\n')\n",
    "        for k,v in colors_tags.items():\n",
    "            file.write(colorize_word(('***',k)))\n",
    "            file.write(f' - {tagset[k]}\\n')\n",
    "            \n",
    "def write_doc(file):\n",
    "    for pi,page in enumerate(tokenaise_text):\n",
    "        file.write(f'<h4>[--- Page {pi} ---]</h3>\\n')\n",
    "        keys = sorted(page.keys())\n",
    "        tag = ''\n",
    "        close_tag = None\n",
    "        for k in keys:\n",
    "            if len(k) == 0: # это на случай ошибки в разметке, когда область даже не показывается во вьювере\n",
    "                continue \n",
    "            tag = '<p>'\n",
    "            if len(k)>1 and k[-1] == 'h':\n",
    "                tag = '<h3>'\n",
    "            elif k[0] == 'f':\n",
    "                tag = f'<p  style=\"background-color: {colors_parts[\"f\"]}\">'\n",
    "                close_tag = '</p>'\n",
    "\n",
    "            with Tag(file, tag, close_tag) as tparagraff:\n",
    "                first_word = True\n",
    "                for sentence in page[k]:\n",
    "                    for word in sentence:\n",
    "                        word_tag = word[1]\n",
    "                        if word_tag != '.' and word_tag != ',':\n",
    "                            first_word = False\n",
    "                            file.write(' ')\n",
    "\n",
    "                        file.write( colorize_word(word) )\n",
    "    \n",
    "\n",
    "with open(f'html_base/Base.html', 'r') as f_html_base:\n",
    "    with open(f'{file_dir_path}/{file_name}.html', 'w') as f_dest:\n",
    "        for line in f_html_base:\n",
    "            if line != '' and line[0] == '@':\n",
    "                if 'legend' in line:\n",
    "                    write_legend(f_dest)\n",
    "                elif 'doc' in line:\n",
    "                    write_doc(f_dest)\n",
    "            else:\n",
    "                f_dest.write(line)\n",
    "                    \n",
    "\n",
    "# with open(f'{file_dir_path}/{file_name}.html', 'w') as f_dest:\n",
    "#     with Tag(f,'<article>\\n') as tart:\n",
    "#         f.write('<p style=\" position: absolute; bottom: 0; left: 0; width: 100%; text-align: center;\">This will stick at the botton no matter what :).</p>')\n",
    "#         for pi,page in enumerate(tokenaise_text):\n",
    "#             f.write(f'<h4>[--- Page {pi} ---]</h3>\\n')\n",
    "#             keys = sorted(page.keys())\n",
    "#             tag = ''\n",
    "#             close_tag = None\n",
    "#             for k in keys:\n",
    "#                 if len(k) == 0: # это на случай ошибки в разметке, когда область даже не показывается во вьювере\n",
    "#                     continue \n",
    "#                 tag = '<p>'\n",
    "#                 if len(k)>1 and k[-1] == 'h':\n",
    "#                     tag = '<h3>'\n",
    "#                 elif k[0] == 'f':\n",
    "#                     tag = f'<p  style=\"background-color: {colors_parts[\"f\"]}\">'\n",
    "#                     close_tag = '</p>'\n",
    "                    \n",
    "#                 with Tag(f, tag, close_tag) as tparagraff:\n",
    "#                     first_word = True\n",
    "#                     for sentence in page[k]:\n",
    "#                         for word in sentence:\n",
    "#                             word_tag = word[1]\n",
    "#                             if word_tag != '.' and word_tag != ',':\n",
    "#                                 first_word = False\n",
    "#                                 f.write(' ')\n",
    "                            \n",
    "#                             f.write( colorize_word(word) )\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
