{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00da8410",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'In our everyday lives, artificial neural networks are already extensively \\\n",
    "used, for example, for carrying out tasks such as face and speech recog- \\\n",
    "nition, which are frequently performed on our mobile phones1. In more \\\n",
    "complex applications, such as medical diagnostics2 and autonomous \\\n",
    "driving1,3, high-speed data analysis will become ever more important. \\\n",
    "However, fulfilling this demand for fast and efficient processing using \\\n",
    "traditional computation techniques is problematic, owing to speed and \\\n",
    "energy inefficiencies4. Traditional computers are built following the \\\n",
    "von Neumann architecture, with two separate units for memory and \\\n",
    "processor operating in a sequential way, one command at a time. When \\\n",
    "compared to the massively parallel signal processing of the brain, it \\\n",
    "is clear why simulating a neural network in software on a machine \\\n",
    "based on the von Neumann architecture and limited by the transfer of \\\n",
    "data between memory and processor cannot be efficient5.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a2c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', \"''\", '(', ')', ',', '--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "\n",
      "r_all==r_by_word = False\n",
      "\n",
      "[('In', 'IN'), ('our', 'PRP$'), ('everyday', 'JJ'), ('lives', 'NNS'), (',', ','), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('are', 'VBP'), ('already', 'RB'), ('extensively', 'RB'), ('used', 'VBN'), (',', ','), ('for', 'IN'), ('example', 'NN'), (',', ','), ('for', 'IN'), ('carrying', 'VBG'), ('out', 'RP'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('face', 'NN'), ('and', 'CC'), ('speech', 'JJ'), ('recog-', 'JJ'), ('nition', 'NN'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('frequently', 'RB'), ('performed', 'VBN'), ('on', 'IN'), ('our', 'PRP$'), ('mobile', 'JJ'), ('phones1', 'NN'), ('.', '.'), ('In', 'IN'), ('more', 'RBR'), ('complex', 'JJ'), ('applications', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('medical', 'JJ'), ('diagnostics2', 'NN'), ('and', 'CC'), ('autonomous', 'JJ'), ('driving1,3', 'NN'), (',', ','), ('high-speed', 'JJ'), ('data', 'NN'), ('analysis', 'NN'), ('will', 'MD'), ('become', 'VB'), ('ever', 'RB'), ('more', 'RBR'), ('important', 'JJ'), ('.', '.'), ('However', 'RB'), (',', ','), ('fulfilling', 'VBG'), ('this', 'DT'), ('demand', 'NN'), ('for', 'IN'), ('fast', 'JJ'), ('and', 'CC'), ('efficient', 'JJ'), ('processing', 'NN'), ('using', 'VBG'), ('traditional', 'JJ'), ('computation', 'NN'), ('techniques', 'NNS'), ('is', 'VBZ'), ('problematic', 'JJ'), (',', ','), ('owing', 'VBG'), ('to', 'TO'), ('speed', 'VB'), ('and', 'CC'), ('energy', 'NN'), ('inefficiencies4', 'NN'), ('.', '.'), ('Traditional', 'JJ'), ('computers', 'NNS'), ('are', 'VBP'), ('built', 'VBN'), ('following', 'VBG'), ('the', 'DT'), ('von', 'JJ'), ('Neumann', 'NNP'), ('architecture', 'NN'), (',', ','), ('with', 'IN'), ('two', 'CD'), ('separate', 'JJ'), ('units', 'NNS'), ('for', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('operating', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sequential', 'JJ'), ('way', 'NN'), (',', ','), ('one', 'CD'), ('command', 'NN'), ('at', 'IN'), ('a', 'DT'), ('time', 'NN'), ('.', '.'), ('When', 'WRB'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('massively', 'RB'), ('parallel', 'JJ'), ('signal', 'JJ'), ('processing', 'NN'), ('of', 'IN'), ('the', 'DT'), ('brain', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('clear', 'JJ'), ('why', 'WRB'), ('simulating', 'VBG'), ('a', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('in', 'IN'), ('software', 'NN'), ('on', 'IN'), ('a', 'DT'), ('machine', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('von', 'NN'), ('Neumann', 'NNP'), ('architecture', 'NN'), ('and', 'CC'), ('limited', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('transfer', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('between', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('efficient5', 'RB'), ('.', '.')]\n",
      "\n",
      "[('In', 'IN'), ('our', 'PRP$'), ('everyday', 'NN'), ('lives', 'NNS'), (',', ','), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('are', 'VBP'), ('already', 'RB'), ('extensively', 'RB'), ('used', 'VBN'), (',', ','), ('for', 'IN'), ('example', 'NN'), (',', ','), ('for', 'IN'), ('carrying', 'VBG'), ('out', 'IN'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('face', 'NN'), ('and', 'CC'), ('speech', 'NN'), ('recog-', 'NN'), ('nition', 'NN'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('frequently', 'RB'), ('performed', 'VBN'), ('on', 'IN'), ('our', 'PRP$'), ('mobile', 'NN'), ('phones1', 'NN'), ('.', '.'), ('In', 'IN'), ('more', 'RBR'), ('complex', 'JJ'), ('applications', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('medical', 'JJ'), ('diagnostics2', 'NN'), ('and', 'CC'), ('autonomous', 'JJ'), ('driving1,3', 'NN'), (',', ','), ('high-speed', 'NN'), ('data', 'NNS'), ('analysis', 'NN'), ('will', 'MD'), ('become', 'NN'), ('ever', 'RB'), ('more', 'RBR'), ('important', 'JJ'), ('.', '.'), ('However', 'RB'), (',', ','), ('fulfilling', 'VBG'), ('this', 'DT'), ('demand', 'NN'), ('for', 'IN'), ('fast', 'NN'), ('and', 'CC'), ('efficient', 'NN'), ('processing', 'NN'), ('using', 'VBG'), ('traditional', 'JJ'), ('computation', 'NN'), ('techniques', 'NNS'), ('is', 'VBZ'), ('problematic', 'JJ'), (',', ','), ('owing', 'VBG'), ('to', 'TO'), ('speed', 'NN'), ('and', 'CC'), ('energy', 'NN'), ('inefficiencies4', 'NN'), ('.', '.'), ('Traditional', 'JJ'), ('computers', 'NNS'), ('are', 'VBP'), ('built', 'NN'), ('following', 'VBG'), ('the', 'DT'), ('von', 'NN'), ('Neumann', 'NN'), ('architecture', 'NN'), (',', ','), ('with', 'IN'), ('two', 'CD'), ('separate', 'JJ'), ('units', 'NNS'), ('for', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('operating', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sequential', 'JJ'), ('way', 'NN'), (',', ','), ('one', 'CD'), ('command', 'NN'), ('at', 'IN'), ('a', 'DT'), ('time', 'NN'), ('.', '.'), ('When', 'WRB'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('massively', 'RB'), ('parallel', 'NN'), ('signal', 'NN'), ('processing', 'NN'), ('of', 'IN'), ('the', 'DT'), ('brain', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('clear', 'JJ'), ('why', 'WRB'), ('simulating', 'VBG'), ('a', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('in', 'IN'), ('software', 'NN'), ('on', 'IN'), ('a', 'DT'), ('machine', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('von', 'NN'), ('Neumann', 'NN'), ('architecture', 'NN'), ('and', 'CC'), ('limited', 'JJ'), ('by', 'IN'), ('the', 'DT'), ('transfer', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('between', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('efficient5', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/nik/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nik/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "  \n",
    "# Если пакет отсутствует, начните по его загрузке\n",
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagdict = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n",
    "# Выведем список тегов\n",
    "print(sorted(tagdict.keys()))\n",
    "print()\n",
    "s = 'It is expected that this type of reaction be used in 21st century. Built. Procesing'\n",
    "s = 'Traditional computers are built following the von Neumann architecture, with two separate units for memory and processor operating in a sequential way, one command at a time'\n",
    "s = 'The synapses are built of optical waveguides'\n",
    "s = text\n",
    "\n",
    "words = nltk.word_tokenize(s)\n",
    "\n",
    "# words = s.split(' ')\n",
    "\n",
    "r_all=nltk.pos_tag(words)\n",
    "\n",
    "r_by_word=[]\n",
    "for w in words:\n",
    "    r_by_word.append(nltk.pos_tag([w])[0])\n",
    "\n",
    "print(f'{r_all==r_by_word = }')\n",
    "print()\n",
    "print(r_all)\n",
    "print()\n",
    "print(r_by_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cacbb2f",
   "metadata": {},
   "source": [
    "\n",
    "#### Для больших текстов эффективнее использовать pos_tag_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "715f0457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('In', 'IN'), ('our', 'PRP$'), ('everyday', 'JJ'), ('lives', 'NNS'), (',', ','), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('are', 'VBP'), ('already', 'RB'), ('extensively', 'RB'), ('used', 'VBN'), (',', ','), ('for', 'IN'), ('example', 'NN'), (',', ','), ('for', 'IN'), ('carrying', 'VBG'), ('out', 'RP'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('face', 'NN'), ('and', 'CC'), ('speech', 'JJ'), ('recog-', 'JJ'), ('nition', 'NN'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('frequently', 'RB'), ('performed', 'VBN'), ('on', 'IN'), ('our', 'PRP$'), ('mobile', 'JJ'), ('phones1', 'NN'), ('.', '.')], [('In', 'IN'), ('more', 'RBR'), ('complex', 'JJ'), ('applications', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('medical', 'JJ'), ('diagnostics2', 'NN'), ('and', 'CC'), ('autonomous', 'JJ'), ('driving1,3', 'NN'), (',', ','), ('high-speed', 'JJ'), ('data', 'NN'), ('analysis', 'NN'), ('will', 'MD'), ('become', 'VB'), ('ever', 'RB'), ('more', 'RBR'), ('important', 'JJ'), ('.', '.')], [('However', 'RB'), (',', ','), ('fulfilling', 'VBG'), ('this', 'DT'), ('demand', 'NN'), ('for', 'IN'), ('fast', 'JJ'), ('and', 'CC'), ('efficient', 'JJ'), ('processing', 'NN'), ('using', 'VBG'), ('traditional', 'JJ'), ('computation', 'NN'), ('techniques', 'NNS'), ('is', 'VBZ'), ('problematic', 'JJ'), (',', ','), ('owing', 'VBG'), ('to', 'TO'), ('speed', 'VB'), ('and', 'CC'), ('energy', 'NN'), ('inefficiencies4', 'NN'), ('.', '.')], [('Traditional', 'JJ'), ('computers', 'NNS'), ('are', 'VBP'), ('built', 'VBN'), ('following', 'VBG'), ('the', 'DT'), ('von', 'JJ'), ('Neumann', 'NNP'), ('architecture', 'NN'), (',', ','), ('with', 'IN'), ('two', 'CD'), ('separate', 'JJ'), ('units', 'NNS'), ('for', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('operating', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sequential', 'JJ'), ('way', 'NN'), (',', ','), ('one', 'CD'), ('command', 'NN'), ('at', 'IN'), ('a', 'DT'), ('time', 'NN'), ('.', '.')], [('When', 'WRB'), ('compared', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('massively', 'RB'), ('parallel', 'JJ'), ('signal', 'JJ'), ('processing', 'NN'), ('of', 'IN'), ('the', 'DT'), ('brain', 'NN'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('clear', 'JJ'), ('why', 'WRB'), ('simulating', 'VBG'), ('a', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('in', 'IN'), ('software', 'NN'), ('on', 'IN'), ('a', 'DT'), ('machine', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('von', 'NN'), ('Neumann', 'NNP'), ('architecture', 'NN'), ('and', 'CC'), ('limited', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('transfer', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('between', 'IN'), ('memory', 'NN'), ('and', 'CC'), ('processor', 'NN'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('efficient5', 'RB'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(text)\n",
    "s = [nltk.word_tokenize(s) for s in sents]\n",
    "print(nltk.pos_tag_sents(s, tagset=None, lang='eng')) # именные параметры приыведены для напоминания, что они есть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3ace3",
   "metadata": {},
   "source": [
    "#### Вычитываем выделенные области из pdf файла\n",
    "\n",
    "text = [page0, page1, ...]   \n",
    "page = {'Пометка из аннотации', 'text'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "91957847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['1h', '2', '3', '4', '5', '6', '7h', '8']\n",
      "1 ['1-pp', '2', '3', '3-p', '5', '6', 'f1', 'f1-p']\n",
      "2 []\n",
      "3 []\n",
      "4 []\n",
      "5 []\n",
      "6 []\n",
      "7 []\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = '10.1038@s41586-019-1157-8 - markup.pdf'\n",
    "file_name = Path(file_path).stem\n",
    "file_dir_path = Path(file_path).parent.resolve()\n",
    "\n",
    "\n",
    "doc = fitz.open(file_path)\n",
    "\n",
    "text = []\n",
    "for i,doc_page in enumerate(doc):\n",
    "    page = {}\n",
    "    for annot in doc_page.annots():\n",
    "        page[annot.info['content']] = doc_page.get_text(clip=annot.rect)\n",
    "    keys = sorted(page.keys())\n",
    "    print(f'{i}', keys)\n",
    "    text.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f0820",
   "metadata": {},
   "source": [
    "#### Объединяем разъединёные части\n",
    "- [ ] части рисунков и обзацев на одной странице\n",
    "- [ ] части абзаца на разных страницах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c1b34a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_i,page in enumerate(text):\n",
    "    keys = sorted(page.keys())\n",
    "    for i,k in enumerate(keys):\n",
    "        if k[-1] == 'p':\n",
    "            if k[-2] == 'p': # part prev page\n",
    "                pass\n",
    "            else: # part prev column\n",
    "                dest_k = k[:-2]\n",
    "                page[dest_k] = page[dest_k] + page[k] \n",
    "            page.pop(k, None)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae274d",
   "metadata": {},
   "source": [
    "#### Токенезируем\n",
    "\n",
    "tokenaise_text = [page0, page1, ...]  \n",
    "page = {'Пометка из аннотации', 'text'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6e43d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenaise_text = []\n",
    "for page in text:\n",
    "    tokenaise_page = {}\n",
    "    keys = sorted(page.keys())\n",
    "    for k in keys:\n",
    "        sents = nltk.sent_tokenize(page[k])\n",
    "        s = [nltk.word_tokenize(s) for s in sents]\n",
    "        tokenaise_page[k] = nltk.pos_tag_sents(s, tagset=None, lang='eng')\n",
    "    tokenaise_text.append(tokenaise_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd44505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenaise_text[0])\n",
    "print(len(tokenaise_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5635ed",
   "metadata": {},
   "source": [
    "#### Экспорт в html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03b2dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1038@s41586-019-1157-8 - markup\n",
      "/home/nik/Work/AVT/Аспирантура/Английский/Translate Helper\n"
     ]
    }
   ],
   "source": [
    "print(file_name)\n",
    "print(file_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d6ff4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag():\n",
    "    def __init__(self, file, open_tag, close_tag = None):\n",
    "        self.file = file\n",
    "        self.open_tag = open_tag\n",
    "        self.close_tag = close_tag\n",
    "        if close_tag is None:\n",
    "            self.close_tag = f'</{self.open_tag[1:]}'\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.file.write(self.open_tag)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.file.write(self.close_tag)\n",
    "\n",
    "colors = {'f': '#fffcde'}        \n",
    "\n",
    "with open(f'{file_dir_path}/{file_name}.html', 'w') as f:\n",
    "    with Tag(f,'<article>\\n') as tart:\n",
    "        for pi,page in enumerate(tokenaise_text):\n",
    "            f.write(f'<h4>[--- Page {pi} ---]</h3>\\n')\n",
    "            keys = sorted(page.keys())\n",
    "            tag = ''\n",
    "            close_tag = None\n",
    "            for k in keys:\n",
    "                tag = '<p>'\n",
    "                if k[-1] == 'h':\n",
    "                    tag = '<h3>'\n",
    "                elif k[0] == 'f':\n",
    "                    tag = f'<p  style=\"background-color: {colors[\"f\"]}\">'\n",
    "                    close_tag = '</p>'\n",
    "                    \n",
    "                with Tag(f, tag, close_tag) as tparagraff:\n",
    "                    first_word = True\n",
    "                    for sentence in page[k]:\n",
    "                        for word in sentence:\n",
    "                            word_tag = word[1]\n",
    "                            if word_tag != '.' and word_tag != ',':\n",
    "                                first_word = False\n",
    "                                f.write(' ')\n",
    "                            f.write(word[0])\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42638f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
